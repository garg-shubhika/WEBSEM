{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WebSem Project: Constructing and Querying a Knowledge Graph in the Cycling Domain: SHUBHIKA GARG\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The goal of this project is to extract information from multilingual textual documents about cycling and create a knowledge graph (KG) using the extracted entities and relations. The KG will be compatible with a cycling ontology and queries will be written in SPARQL to retrieve specific information from the KG. The project will be implemented using Jupyter Notebook and the following steps will be followed:\n",
        "\n",
        "* Collect multilingual textual documents about cycling\n",
        "* Pre-process the documents to get clean txt files\n",
        "* Run named entity recognition (NER) on the documents to extract named entities of the type Person, Organization and Location using spaCy\n",
        "* Run co-reference resolution on the input text using spaCy\n",
        "* Disambiguate the entities with Wikidata using OpenTapioca\n",
        "* Run relation extraction using Stanford OpenIE\n",
        "* Implement some mappings between the entity types and relations returned with the cycling ontology you developed during the Assignment 1 in order to create a knowledge graph of the domain represented in RDF\n",
        "* Load the data in the Corese engine as you did for the Assignment 2 with your cycling ontology and the knowledge graph built in the previous step and write some SPARQL queries to retrieve specific information from the KG\n",
        "\n",
        "### Useful resources\n",
        "* The github repository \"Building knowledge graph from input data\" at  https://github.com/varun196/knowledge_graph_from_unstructured_text can be used as an inspiration\n",
        "\n",
        "### References\n",
        "* NLTK: https://www.nltk.org/\n",
        "* spaCy: https://spacy.io/\n",
        "* Stanford OpenIE: https://nlp.stanford.edu/software/openie.html\n",
        "* OpenTapioca: https://opentapioca.org/\n",
        "* Corese engine: https://project.inria.fr/corese/\n",
        "* Wikidata: https://www.wikidata.org/"
      ],
      "metadata": {
        "id": "mpecDDB_GGLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Collect multilingual textual documents about cycling\n",
        "For this mini project, we will collect multilingual textual documents about cycling from various sources such as news articles, blog posts, and race reports. We will download the documents and save them in a directory called `cycling_docs`.\n",
        "\n",
        "The list of documents to download are available at:\n",
        "\n",
        "* English:\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21\n",
        " - https://www.bbc.com/sport/cycling/61940037\n",
        " - https://www.bbc.com/sport/cycling/62017114 (stage 1)\n",
        " - https://www.bbc.com/sport/cycling/62097721 (stage 7)\n",
        " - https://www.bbc.com/sport/cycling/62153759 (stage 11)\n",
        " - https://www.bbc.co.uk/sport/cycling/62285420 (stage 21)\n",
        "\n",
        "* French:\n",
        " - https://fr.wikipedia.org/wiki/Tour_de_France_2022\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html"
      ],
      "metadata": {
        "id": "EzMinLFxGUFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to install more dependencies if needed!\n",
        "\n",
        "# Install jusText for automatically extracting text from web pages\n",
        "!pip install --quiet jusText\n",
        "\n",
        "# Install nltk for text processing\n",
        "!pip install --quiet nltk\n",
        "\n",
        "# Install spaCy for NER extraction\n",
        "!pip install --quiet spacy\n",
        "\n",
        "# Install pycorenlp for Stanford CoreNLP\n",
        "!pip install --quiet pycorenlp\n",
        "\n",
        "# Install pandas for data visualization\n",
        "!pip install pandas\n",
        "\n",
        "# Install rdflib for writing RDF\n",
        "!pip install rdflib"
      ],
      "metadata": {
        "id": "HccrPk8uGVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import requests\n",
        "import justext\n",
        "import os\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "\n",
        "# Define function to get filename from URL\n",
        "def get_filename_from_url(url):\n",
        "  urlpath = urlsplit(url).path\n",
        "  return os.path.basename(urlpath)\n",
        "\n",
        "\n",
        "# Define function to download URLs and extract text\n",
        "def download_urls(urls_list, language):\n",
        "  # Loop over each URL in the list\n",
        "  for url in urls_list:\n",
        "    # Fetch and extract text from the URL using jusText\n",
        "    response = requests.get(url)\n",
        "    paragraphs = justext.justext(\n",
        "      response.content,\n",
        "      justext.get_stoplist(language.capitalize()),\n",
        "      no_headings=True,\n",
        "      max_heading_distance=150,\n",
        "      length_low=70,\n",
        "      length_high=140,\n",
        "      stopwords_low=0.2,\n",
        "      stopwords_high=0.3,\n",
        "      max_link_density=0.4\n",
        "    )\n",
        "    extracted_text = '\\n'.join(list(filter(None, map(lambda paragraph: paragraph.text if not paragraph.is_boilerplate else '', paragraphs))))\n",
        "\n",
        "    # Truncate text\n",
        "    extracted_text = extracted_text[0:10000]\n",
        "\n",
        "    # Save extracted text as a .txt file\n",
        "    filename = get_filename_from_url(url)\n",
        "    output_path = os.path.join('cycling_docs', f'{filename}.{language}.txt')\n",
        "    with open(output_path, 'w') as f:\n",
        "      f.write(extracted_text)\n",
        "    # Print a message to show that the URL was downloaded and saved\n",
        "    print(f'Downloaded {url} into {output_path}')\n",
        "\n",
        "\n",
        "# List of URLs to download\n",
        "urls_list_english = [\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21',\n",
        "  'https://www.bbc.com/sport/cycling/61940037',\n",
        "  'https://www.bbc.com/sport/cycling/62017114',\n",
        "  'https://www.bbc.com/sport/cycling/62097721',\n",
        "  'https://www.bbc.com/sport/cycling/62153759',\n",
        "  'https://www.bbc.co.uk/sport/cycling/62285420',\n",
        "]\n",
        "urls_list_french = [\n",
        "  'https://fr.wikipedia.org/wiki/Tour_de_France_2022',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html',\n",
        "]\n",
        "\n",
        "# Create the output directory if it does not exist\n",
        "os.makedirs('cycling_docs', exist_ok=True)\n",
        "\n",
        "download_urls(urls_list_english, 'english')\n",
        "download_urls(urls_list_french, 'french')"
      ],
      "metadata": {
        "id": "_031XjozIHqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Pre-process the documents to get clean txt files\n",
        "We will pre-process the documents to get clean txt files by removing any unnecessary characters, punctuation, and stopwords. We will use Python's [re](https://docs.python.org/3/library/re.html) and [nltk](https://www.nltk.org/) libraries for this purpose. We will save the results in a `clean_docs` folder."
      ],
      "metadata": {
        "id": "Wg52A5ELGWvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: Import the necessary libraries for natural language processing\n",
        "import os\n",
        "#Importing the necessary libraries\n",
        "import os\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(dirty_text, language):\n",
        "    # Remove any unnecessary characters and punctuation\n",
        "    cleaned_text = re.sub(\"[^0-9A-Za-z ]\", \"\", dirty_text)\n",
        "    \n",
        "    # Convert the text to lowercase\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    \n",
        "    # Tokenize the words\n",
        "    words = word_tokenize(cleaned_text, language=language)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    filtered_words = [word for word in words if not word in stop_words]\n",
        "    cleaned_text = str(filtered_words)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "k9JsLAGsxsyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to process a file and write the result to a new file\n",
        "def process_file(file):\n",
        "  # Open the file in read-only mode and read all of its lines\n",
        "  with open(file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  # Concatenate all the lines into a single string\n",
        "  raw_text = '\\n'.join(lines)\n",
        "\n",
        "  # Determine the language of the file based on its extension\n",
        "  language = 'french' if file.endswith('.french.txt') else 'english'\n",
        "\n",
        "  # Clean the text using the clean_text function\n",
        "  cleaned_text = clean_text(raw_text, language)\n",
        "\n",
        "  # Create the output directory if it does not exist\n",
        "  os.makedirs('clean_docs', exist_ok=True)\n",
        "\n",
        "  # Write the cleaned text into a new file\n",
        "  with open(os.path.join('clean_docs', os.path.basename(file)), 'w') as out:\n",
        "    out.write(cleaned_text)\n",
        "\n",
        "\n",
        "# Loop through all the files in the \"cycling_docs\" folder\n",
        "folder = 'cycling_docs'\n",
        "for filename in os.listdir(folder):\n",
        "  # Construct the full path to the file\n",
        "  file = os.path.join(folder, filename)\n",
        "\n",
        "  # Check if the file is a regular file and has a .txt extension\n",
        "  if os.path.isfile(file) and file.endswith('.txt'):\n",
        "    # Call the process_file function on the file\n",
        "    process_file(file)"
      ],
      "metadata": {
        "id": "KhFqE5TleLz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Run named entity recognition (NER) on the documents\n",
        "We will use [spaCy](https://spacy.io)'s pre-trained models to perform NER on the documents and extract the entities of type PER/ORG/LOC. We will save the extracted entities in a file."
      ],
      "metadata": {
        "id": "nXH__c_rGaIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two lists, one for French documents and one for English documents\n",
        "import os\n",
        "docs_fr = []\n",
        "docs_en = []\n",
        "\n",
        "# üìù TODO: Loop through all the files in the \"clean_docs\" folder and append their\n",
        "#    content into `docs_fr` and `docs_en` lists. The end result should look like:\n",
        "#    `docs_en = [ \"doc 1 text...\", \"doc 2 text...\", \"doc 3 text...\", ...]`.\n",
        "\n",
        "# Loop through all the files in the \"clean_docs\" folder and append their\n",
        "# content into `docs_fr` and `docs_en` lists\n",
        "for i, filename in enumerate(os.listdir('clean_docs')):\n",
        "    # Construct the full path to the file\n",
        "    file = os.path.join('clean_docs', filename)\n",
        "\n",
        "    # Read the contents of the file\n",
        "    with open(file, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Determine the language of the file based on its extension\n",
        "    language = 'french' if file.endswith('.french.txt') else 'english'\n",
        "\n",
        "\n",
        "    # Append the text to the appropriate list based on the language\n",
        "    if language == 'french':\n",
        "        docs_fr.append(text)\n",
        "    elif language == 'english':\n",
        "        docs_en.append(text)"
      ],
      "metadata": {
        "id": "pl2G3gW21Wsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first document for each language\n",
        "display(docs_fr[0])\n",
        "display(docs_en[0])"
      ],
      "metadata": {
        "id": "5Lk-A5CFB-2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "joPa7r6TOYz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "aQnmcwmXOx2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "# Load spaCy models for French and English\n",
        "nlp_fr = spacy.load('fr_core_news_sm')\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract entities from text\n",
        "def extract_entities(text, language):\n",
        "    # Select the appropriate spaCy model based on the language\n",
        "    nlp = nlp_fr if language == 'french' else nlp_en\n",
        "    # Apply the spaCy model to the text\n",
        "    doc = nlp(text)\n",
        "    # Extract the named entities and their labels\n",
        "    entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents if ent.label_ in ['PER', 'ORG', 'LOC']]\n",
        "    # Return extracted entities\n",
        "    return entities\n",
        "\n",
        "# Create two dictionaries to store entities for French and English documents\n",
        "entities_fr = {}\n",
        "entities_en = {}\n",
        "\n",
        "# Loop through all the files in the \"clean_docs\" folder and extract entities\n",
        "for filename in os.listdir('clean_docs'):\n",
        "    # Construct the full path to the file\n",
        "    file = os.path.join('clean_docs', filename)\n",
        "\n",
        "    # Determine the language of the file based on its extension\n",
        "    language = 'french' if file.endswith('.french.txt') else 'english'\n",
        "\n",
        "    # Read the contents of the file\n",
        "    with open(file, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Extract entities from the text\n",
        "    entities = extract_entities(text, language)\n",
        "\n",
        "    # Add the entities to the appropriate dictionary based on the language\n",
        "    if language == 'french':\n",
        "        entities_fr[filename] = entities\n",
        "    elif language == 'english':\n",
        "        entities_en[filename] = entities\n",
        "\n",
        "# Save the extracted entities to files\n",
        "with open('entities_fr.json', 'w') as f:\n",
        "    json.dump(entities_fr, f)\n",
        "\n",
        "with open('entities_en.json', 'w') as f:\n",
        "    json.dump(entities_en, f)\n"
      ],
      "metadata": {
        "id": "MkMYXqVFGy9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract entities for each document\n",
        "entities_fr = []\n",
        "entities_en = []\n",
        "\n",
        "for text in docs_fr:\n",
        "  entities_fr.append(extract_entities(text, 'fr'))\n",
        "\n",
        "for text in docs_en:\n",
        "  entities_en.append(extract_entities(text, 'en'))"
      ],
      "metadata": {
        "id": "REq50-k7ePSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display entities which have been extracted"
      ],
      "metadata": {
        "id": "NkFvU6OfDwux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: For each language, display extracted entities for the first document\n",
        "# Display extracted entities for the first document in each language\n",
        "# Extract entities for each document\n",
        "entities_fr = []\n",
        "entities_en = []\n",
        "\n",
        "for text in docs_fr:\n",
        "  entities_fr.append(extract_entities(text, 'french'))\n",
        "\n",
        "for text in docs_en:\n",
        "  entities_en.append(extract_entities(text, 'english'))\n",
        "\n",
        "# Display extracted entities for the first document in each language\n",
        "print(\"French entities for the first document:\", entities_fr[0])\n",
        "print(\"English entities for the first document:\", entities_en[0])\n"
      ],
      "metadata": {
        "id": "ER2xsq4WYJgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Run co-reference resolution on the input text\n",
        "We will use CoreNLP to perform [co-reference resolution](https://en.wikipedia.org/wiki/Coreference) on the input text and resolve coreferences.\n",
        "\n",
        "For this project, we will use a hosted version of CoreNLP at: https://corenlp.tools.eurecom.fr/. Feel free to try out the web interface before writing the code."
      ],
      "metadata": {
        "id": "lAsxblm4GceJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "from tqdm import tqdm\n",
        "# Set up the CoreNLP client\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "def resolve_coreferences(text, language):\n",
        "    output = nlp.annotate(text, properties={\n",
        "        'timeout': 300000,\n",
        "        'annotators': 'tokenize,ssplit,coref',\n",
        "        'pipelineLanguage': language,\n",
        "        'outputFormat': 'json'\n",
        "    })\n",
        "    \n",
        "\n",
        "    try:\n",
        "        output = json.loads(output)\n",
        "    except Exception as err:\n",
        "        print(f'Unexpected response: {output}')\n",
        "        raise\n",
        "            \n",
        "        # Extract the resolved text\n",
        "        corefs = output['corefs']\n",
        "        for _, i in corefs.items():\n",
        "            # Find the most representative mention\n",
        "            for j in i:\n",
        "                if mention['isRepresentativeMention']:\n",
        "                    target_text = j['text']\n",
        "            \n",
        "            # Replace co-reference with resolved value\n",
        "            for j in i:\n",
        "              if not j['isRepresentativeMention']:\n",
        "                x = j['startIndex']-1\n",
        "                y = j['endIndex']-1\n",
        "                text = text[:x] + target_text + text[y:]\n",
        "\n",
        "    \n",
        "    return text\n"
      ],
      "metadata": {
        "id": "v0WOTNW3Ggg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[len(docs_en[i]) for i in range(8)]"
      ],
      "metadata": {
        "id": "HO-EsHSnYvRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs_en1 = resolve_coreferences(docs_en[1], 'en')\n",
        "#docs_en1"
      ],
      "metadata": {
        "id": "GyJfVaseaRC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list(docs_en1)"
      ],
      "metadata": {
        "id": "UTTXupDTbyBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(docs_en)):\n",
        "  print(f'Processing document {i}')\n",
        "  try:\n",
        "    docs_en[i] = resolve_coreferences(docs_en[i], 'en')\n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "V4P_mgqCeebD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs_en_5 = resolve_coreferences(docs_en[5], 'en')"
      ],
      "metadata": {
        "id": "8CVIhfTmen7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Disambiguate the entities with Wikidata using OpenTapioca\n",
        "We will use [OpenTapioca](https://opentapioca.org/) to disambiguate the entities with Wikidata and retrieve their unique identifiers (QIDs)."
      ],
      "metadata": {
        "id": "oIQZdbp9Gg6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Define the API endpoint URL\n",
        "opentapioca_url = 'https://opentapioca.wordlift.io/api/annotate'\n",
        "\n",
        "def opentapioca_annotate(text, language):\n",
        "  # Define the request parameters\n",
        "  params = {\n",
        "    'query': text,\n",
        "    'lang': language\n",
        "  }\n",
        "\n",
        "  # Send the GET request to the OpenTapioca API endpoint\n",
        "  response = requests.get(opentapioca_url, params=params)\n",
        "\n",
        "  # üìù TODO: Extract the entities from the API response object\n",
        "  #    `entities = ...`\n",
        "  # üí° You can start by printing the `response` object to understand its structure.\n",
        "  entities = []\n",
        "  try:\n",
        "      response_json = response.json()\n",
        "      annotations = response_json['annotations']\n",
        "      for annotation in annotations:\n",
        "          label = annotation.get('label', 'Unknown')\n",
        "          uri = annotation.get('uri', '')\n",
        "          entity = {'label': label, 'uri': uri}\n",
        "          entities.append(entity)\n",
        "  except KeyError:\n",
        "      print('No annotations found in response') \n",
        "  except Exception as err:\n",
        "      print(f'Unexpected response: {response.text}')\n",
        "      raise\n",
        "  \n",
        "    # Return entities\n",
        "  return entities"
      ],
      "metadata": {
        "id": "CUkHtwaqTDTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_entities_fr = []\n",
        "wiki_entities_en = []\n",
        "\n",
        "for text in docs_fr:\n",
        "  entities = {}\n",
        "  for j in range(0, len(text), 4000):\n",
        "    entities |= opentapioca_annotate(text[j:j+4000], 'fr')\n",
        "  wiki_entities_fr.append(entities)\n",
        "\n",
        "for text in docs_en:\n",
        "  entities = {}\n",
        "  for j in range(0, len(text), 4000):\n",
        "    entities |= opentapioca_annotate(text[j:j+4000], 'en')\n",
        "  wiki_entities_en.append(entities)"
      ],
      "metadata": {
        "id": "3v2QQmase3CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display extracted Wikidata entities"
      ],
      "metadata": {
        "id": "5cC1zoDSe5BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: For each language, display extracted Wikidata entities\n",
        "print(\"Extracted entities for French documents:\")\n",
        "for x, entities in enumerate(wiki_entities_fr):\n",
        "    print(f\"Document {x}: {entities}\")\n",
        "    \n",
        "print(\"Extracted entities for English documents:\")\n",
        "for x, entities in enumerate(wiki_entities_en):\n",
        "    print(f\"Document {x}: {entities}\")"
      ],
      "metadata": {
        "id": "B80_ZKM7SoIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Run relation extraction using Stanford OpenIE\n",
        "We will use Stanford OpenIE to extract the relations between the entities in the input text."
      ],
      "metadata": {
        "id": "fNvv056SGirj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "# Create a StanfordCoreNLP object\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "# Define a function to extract relations from input text using Stanford OpenIE\n",
        "def extract_relations(input_text, language):\n",
        "  output = nlp.annotate(input_text, properties={\n",
        "    'timeout': 300000,\n",
        "    'annotators': 'tokenize,ssplit,openie',\n",
        "    'outputFormat': 'json',\n",
        "    'pipelineLanguage': language\n",
        "  })\n",
        "  try:\n",
        "    output = json.loads(output)\n",
        "  except Exception as err:\n",
        "    print(f'Unexpected response: {output}')\n",
        "    raise\n",
        "\n",
        "  # üìù TODO: Get relations from the `output` object (subject, relation, object)\n",
        "  #    and append them to a `relations` list.\n",
        "  # üí° You can start by printing the `output` object to understand its structure.\n",
        "  relations = []\n",
        "  for sentence in output['sentences']:\n",
        "      for openie in sentence['openie']:\n",
        "          relation = {\n",
        "              'subject': openie['subject'],\n",
        "              'relation': openie['relation'],\n",
        "              'object': openie['object']\n",
        "          }\n",
        "          relations.append(relation)\n",
        "\n",
        "  # Return relations\n",
        "  return relations"
      ],
      "metadata": {
        "id": "mVVKYu3CGjaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relations_en = []\n",
        "for i in range(len(docs_en)):\n",
        "  print(f'Processing document {i}')\n",
        "  relations_en.append(extract_relations(docs_en[i], 'en'))"
      ],
      "metadata": {
        "id": "Wa4T96f1f2Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Implement some mappings between the entity types and relations returned with a given cycling ontology\n",
        "We will implement mappings between the entity types and relations returned with the cycling ontology available at https://www.eurecom.fr/~troncy/teaching/websem2023/cycling.owl."
      ],
      "metadata": {
        "id": "A_YdNyfHGlC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rdflib\n",
        "\n",
        "g = rdflib.Graph()\n",
        "\n",
        "# üìù TODO: Create an RDF graph based on the cycling ontology and using the data\n",
        "# ¬†  from `relations_en`, `entities_en`, and `wiki_entities_en`.\n",
        "# Define namespaces for the entities and relations\n",
        "CYCLING = rdflib.Namespace('https://www.eurecom.fr/~troncy/teaching/websem2023/cycling.owl#')\n",
        "SCHEMA = rdflib.Namespace('http://schema.org/')\n",
        "g.bind('cycling', CYCLING)\n",
        "g.bind('schema', SCHEMA)\n",
        "entities_en = {\n",
        "    'Chris Froome': 'http://dbpedia.org/resource/Chris_Froome',\n",
        "    'Team Sky': 'http://dbpedia.org/resource/Team_Sky',\n",
        "    'Richie Porte': 'http://dbpedia.org/resource/Richie_Porte',\n",
        "    '2016 Tour de France': 'http://dbpedia.org/resource/2016_Tour_de_France',\n",
        "    'Colombia': 'http://dbpedia.org/resource/Colombia',\n",
        "    'Esteban Chaves': 'http://dbpedia.org/resource/Esteban_Chaves',\n",
        "    'Giro d\\'Italia': 'http://dbpedia.org/resource/Giro_d\\'Italia',\n",
        "    'Vuelta a Espa√±a': 'http://dbpedia.org/resource/Vuelta_a_Espa√±a'\n",
        "}\n",
        "# üìù TODO: Create an RDF graph based on the cycling ontology and using the data\n",
        "#    from `relations_en`, `entities_en`, and `wiki_entities_en`.\n",
        "# Define the relations\n",
        "relations_en = {\n",
        "    'ride': CYCLING.ride,\n",
        "    'isPartOf': CYCLING.isPartOf,\n",
        "    'win': CYCLING.win,\n",
        "    'rank': CYCLING.rank,\n",
        "    'team': CYCLING.team,\n",
        "    'participant': CYCLING.participant,\n",
        "    'location': CYCLING.location,\n",
        "    'belongsTo': CYCLING.belongsTo,\n",
        "    'wikipedia': SCHEMA.about\n",
        "}\n",
        "\n",
        "# Define the entities with their DBpedia URLs\n",
        "wiki_entities_en = {}\n",
        "for name, url in entities_en.items():\n",
        "    wiki_entities_en[name] = rdflib.URIRef(url)\n",
        "\n",
        "# Bind the entity and relation namespaces to the graph\n",
        "g.bind('cycling', CYCLING)\n",
        "g.bind('schema', SCHEMA)\n",
        "\n",
        "# Add the entities to the graph\n",
        "for entity in wiki_entities_en.values():\n",
        "    g.add((entity, SCHEMA.name, rdflib.Literal(entity.split('/')[-1].replace('_', ' '))))\n",
        "    g.add((entity, SCHEMA.url, rdflib.Literal(entity)))\n",
        "\n",
        "# Add the relations to the graph\n",
        "for name, relation in relations_en.items():\n",
        "    g.add((relation, SCHEMA.name, rdflib.Literal(name)))"
      ],
      "metadata": {
        "id": "uwo01wgOGmMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result into a file\n",
        "g.serialize(destination='output.ttl')"
      ],
      "metadata": {
        "id": "Nq4jHBEsUxCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Load the data in the Corese engine with the ontology and write the SPARQL queries to retrieve specific information from the KG\n",
        "We will load the data in the [Corese](https://www.eurecom.fr/~troncy/teaching/websem2023/corese-3.2.3c.jar) engine (the same you used in the Assignment 2) with the ontology and write the SPARQL queries to retrieve specific information from the KG. We will write the following queries:"
      ],
      "metadata": {
        "id": "VJXcKJUZGmqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* üìù List the name of the cycling teams"
      ],
      "metadata": {
        "id": "cSLn1iZB3BUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PREFIX cycling: <https://www.eurecom.fr/~troncy/teaching/websem2023/cycling.owl#>\n",
        "#PREFIX schema: <http://schema.org/>\n",
        "\n",
        "#SELECT DISTINCT ?x\n",
        "#WHERE {\n",
        "#  ?y a cycling:Team ;\n",
        "#        schema:name ?x .\n",
        "#}"
      ],
      "metadata": {
        "id": "1Te8aSQkcqrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* üìù List the name of the cycling riders"
      ],
      "metadata": {
        "id": "K0ITJ6LO3C9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PREFIX cycling: <https://www.eurecom.fr/~troncy/teaching/websem2023/cycling.owl#>\n",
        "#PREFIX schema: <http://schema.org/>\n",
        "\n",
        "#SELECT DISTINCT ?n\n",
        "#WHERE {\n",
        "#  ?r a cycling:Participant ;\n",
        "#         cycling:ride ?race ;\n",
        "#         schema:name ?n .\n",
        "#}\n"
      ],
      "metadata": {
        "id": "NaKzLJlHdAPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* üìù Retrieve the name of the winner of the Prologue"
      ],
      "metadata": {
        "id": "t5u5FyOq3FjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PREFIX cycling: <https://www.eurecom.fr/~troncy/teaching/websem2023/cycling.owl#>\n",
        "#PREFIX schema: <http://schema.org/>\n",
        "\n",
        "#SELECT ?winner\n",
        "#WHERE {\n",
        "#  ?prologue a cycling:Prologue ;\n",
        "#            cycling:belongsTo ?race ;\n",
        "#            cycling:win ?rider .\n",
        "#  ?rider schema:name ?winner .\n",
        "#  FILTER (?race = cycling:Prologue)\n",
        "#}"
      ],
      "metadata": {
        "id": "9XGHvAOMdXmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù We will also write the same 3 queries on Wikidata starting from `Q98043180` to compare the results."
      ],
      "metadata": {
        "id": "kRpBPPYR3HTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SELECT DISTINCT ?teamLabel WHERE {\n",
        "#  wd:Q98043180 wdt:P1923 ?team.\n",
        "#  ?team rdfs:label ?teamLabel.\n",
        "#  FILTER((LANG(?teamLabel)) = \"en\")\n",
        "#  }"
      ],
      "metadata": {
        "id": "YpeHco5kh7tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SELECT DISTINCT ?teamLabel WHERE {\n",
        "#  wd:Q98043180 wdt:P710 ?team.\n",
        "#  ?team rdfs:label ?teamLabel.\n",
        "#  FILTER (lang(?teamLabel) = \"en\")\n",
        "#}\n",
        "#ORDER BY ?teamLabel"
      ],
      "metadata": {
        "id": "lsrwiKCNiAEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SELECT ?riderLabel WHERE {\n",
        "#  wd:Q920285 wdt:P1346 ?rider.\n",
        "#  ?rider wdt:P1344 wd:Q98043180.\n",
        "#  ?rider rdfs:label ?riderLabel.\n",
        "#  FILTER(LANG(?riderLabel) = \"en\")\n",
        "#}\n"
      ],
      "metadata": {
        "id": "Dw02ejIulHIk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}